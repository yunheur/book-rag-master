{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHFlxxuZVZtW"
   },
   "outputs": [],
   "source": [
    "pip install langchain langchain_openai langchain_community chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# 먼저 구글 드라이브 마운트\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiklTkHWU_t5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일에서 환경 변수 로드\n",
    "load_dotenv(\"/content/.env\")\n",
    "\n",
    "# 환경 변수에서 API 키 가져오기\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autAFd8SpZRf"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 문서 로더 설정\n",
    "loaders = [TextLoader(\"/content/drive/MyDrive/langchain-tutorial/Ch04. Advanced Rag/Data/How_to_invest_money.txt\")]\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmbIHqDqWM2j"
   },
   "outputs": [],
   "source": [
    "# 문서 생성을 위한 텍스트 분할기 정의\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# 문서 분할\n",
    "split_docs = recursive_splitter.split_documents(docs)\n",
    "\n",
    "# OpenAIEmbeddings 인스턴스 생성\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Chroma vectorstore 생성\n",
    "vectorstore = Chroma.from_documents(documents=split_docs, embedding=embeddings)\n",
    "\n",
    "# Chroma vectorstore 기반 리트리버 생성\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LS5FqQo_ZnIg"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "\n",
    "# 1. 가상 문서 생성 체인\n",
    "def create_virtual_doc_chain():\n",
    "    system = \"당신은 고도로 숙련된 AI입니다.\"\n",
    "    user = \"\"\"\n",
    "    주어진 질문 '{query}'에 대해 직접적으로 답변하는 가상의 문서를 생성하세요.\n",
    "    문서의 크기는 {chunk_size} 글자 언저리여야 합니다.\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system),\n",
    "        (\"human\", user)\n",
    "    ])\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "    return prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mM2cglOWZoUp"
   },
   "outputs": [],
   "source": [
    "# 2. 문서 검색 체인\n",
    "def create_retrieval_chain():\n",
    "    return RunnableLambda(lambda x: retriever.get_relevant_documents(x['virtual_doc']))\n",
    "\n",
    "# 유틸리티 함수\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mPFshxLZqFQ"
   },
   "outputs": [],
   "source": [
    "# 3. 최종 응답 생성 체인\n",
    "def create_final_response_chain():\n",
    "    final_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    다음 정보와 질문을 바탕으로 답변해주세요:\n",
    "\n",
    "    컨텍스트: {context}\n",
    "\n",
    "    질문: {question}\n",
    "\n",
    "    답변:\n",
    "    \"\"\")\n",
    "    final_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "    return final_prompt | final_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDiqFURKVVNE"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def print_input_output(input_data, output_data, step_name):\n",
    "    print(f\"\\n--- {step_name} ---\")\n",
    "    print(f\"Input: {input_data}\")\n",
    "    print(f\"Output: {output_data}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjGXof_qe8qo"
   },
   "outputs": [],
   "source": [
    "def create_pipeline_with_logging():\n",
    "    virtual_doc_chain = create_virtual_doc_chain()\n",
    "    retrieval_chain = create_retrieval_chain()\n",
    "    final_response_chain = create_final_response_chain()\n",
    "\n",
    "    # 가상 문서 생성 단계\n",
    "    def virtual_doc_step(x):\n",
    "        result = {\"virtual_doc\": virtual_doc_chain.invoke({\n",
    "            \"query\": x[\"question\"],\n",
    "            \"chunk_size\": 200\n",
    "        })}\n",
    "        print_input_output(x, result, \"Virtual Doc Generation\")\n",
    "        return {**x, **result}\n",
    "\n",
    "    # 문서 검색 단계\n",
    "    def retrieval_step(x):\n",
    "        result = {\"retrieved_docs\": retrieval_chain.invoke(x)}\n",
    "        print_input_output(x, result, \"Document Retrieval\")\n",
    "        return {**x, **result}\n",
    "\n",
    "    # 컨텍스트 포맷팅 단계\n",
    "    def context_formatting_step(x):\n",
    "        result = {\"context\": format_docs(x[\"retrieved_docs\"])}\n",
    "        print_input_output(x, result, \"Context Formatting\")\n",
    "        return {**x, **result}\n",
    "\n",
    "    # 최종 응답 생성 단계\n",
    "    def final_response_step(x):\n",
    "        result = final_response_chain.invoke(x)\n",
    "        print_input_output(x, result, \"Final Response Generation\")\n",
    "        return result\n",
    "\n",
    "    # 전체 파이프라인 구성\n",
    "    pipeline = (\n",
    "        RunnableLambda(virtual_doc_step)\n",
    "        | RunnableLambda(retrieval_step)\n",
    "        | RunnableLambda(context_formatting_step)\n",
    "        | RunnableLambda(final_response_step)\n",
    "    )\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "# 파이프라인 객체 생성\n",
    "pipeline = create_pipeline_with_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRY2tdLkyLTI"
   },
   "outputs": [],
   "source": [
    "# 예시 질문과 답변\n",
    "question = \"주식 시장의 변동성이 높을 때 투자 전략은 무엇인가요?\"\n",
    "response = pipeline.invoke({\"question\": question})\n",
    "print(f\"최종 답변: {response.content}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM8K4G83HrX4KuKmgUo7ZMZ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
