{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3891,
     "status": "ok",
     "timestamp": 1741004880874,
     "user": {
      "displayName": "ì¡°ê²½ì•„",
      "userId": "05652149574770065967"
     },
     "user_tz": -540
    },
    "id": "Ojlsc5Izydm6",
    "outputId": "c0adfdc2-168d-4ca7-aa69-f50e916a1f25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "PeY1QEffyjJp"
   },
   "outputs": [],
   "source": [
    "# %%capture --no-stderr\n",
    "# !pip install python-dotenv langchain_openai langchain-chroma pypdf langchain langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvSfsBH6yjHD"
   },
   "outputs": [],
   "source": [
    "# í™˜ê²½ë³€ìˆ˜ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-KGYBfqCyjEY"
   },
   "outputs": [],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv(\"/content/.env\")\n",
    "# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "m6GqDTFCaldW"
   },
   "outputs": [],
   "source": [
    "# <2024 ë¶€ë™ì‚° ë³´ê³ ì„œ RAG ì±—ë´‡>\n",
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20215,
     "status": "ok",
     "timestamp": 1741001020404,
     "user": {
      "displayName": "ì¡°ê²½ì•„",
      "userId": "05652149574770065967"
     },
     "user_tz": -540
    },
    "id": "567kMYTY_RK7",
    "outputId": "cc7a009f-8955-4842-a8b3-1c5061d8f4e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¶„í• ëœ ì²­í¬ì˜ ìˆ˜: 135\n"
     ]
    }
   ],
   "source": [
    "# PDF ë¬¸ì„œ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë¶„í• \n",
    "# loader = PyPDFLoader(\"/content/drive/MyDrive/langchain-tutorial/Ch02. RAG/Data/2024_KB_ë¶€ë™ì‚°_ë³´ê³ ì„œ_ìµœì¢….pdf\")\n",
    "loader = PyPDFLoader(\"./Data/2024_KB_ë¶€ë™ì‚°_ë³´ê³ ì„œ_ìµœì¢….pdf\")\n",
    "documents = loader.load()  # ë¬¸ì„œ ë¡œë“œ\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •: ì²­í¬ í¬ê¸°ì™€ ê²¹ì¹¨ ì„¤ì •\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# ë¶„í• ëœ ì²­í¬ ìˆ˜\n",
    "print('ë¶„í• ëœ ì²­í¬ì˜ ìˆ˜:', len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Mxnoi4tDYJWg"
   },
   "outputs": [],
   "source": [
    "# ì„ë² ë”© ìƒì„± ë° Chroma ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥\n",
    "embedding_function = OpenAIEmbeddings()  # ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "\n",
    "# persist_directory = \"/content/drive/MyDrive/langchain-tutorial/Ch02. RAG/directory/chroma\"\n",
    "persist_directory = \"./directory/chroma\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=persist_directory  # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²½ë¡œ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1741001027452,
     "user": {
      "displayName": "ì¡°ê²½ì•„",
      "userId": "05652149574770065967"
     },
     "user_tz": -540
    },
    "id": "CVX9CSgMARiy",
    "outputId": "86c91e0b-46f6-4cbe-d068-6d412b3a86c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ì„œì˜ ìˆ˜: 135\n"
     ]
    }
   ],
   "source": [
    "print('ë¬¸ì„œì˜ ìˆ˜:', vectorstore._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PHVOBLLscEVy"
   },
   "outputs": [],
   "source": [
    "# ê²€ìƒ‰ ë° ì¬ì •ë ¬\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})  # ê´€ë ¨ ë¬¸ì„œ ìƒìœ„ 3ê°œ ê²€ìƒ‰ ì„¤ì •\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •: ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ê¸° ìœ„í•œ í…œí”Œë¦¿\n",
    "template = \"\"\"ë‹¹ì‹ ì€ KB ë¶€ë™ì‚° ë³´ê³ ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ì»¨í…ìŠ¤íŠ¸: {context}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", template),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")  # í…œí”Œë¦¿ ì´ˆê¸°í™”\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)  # AI ëª¨ë¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83,
     "status": "ok",
     "timestamp": 1741001027684,
     "user": {
      "displayName": "ì¡°ê²½ì•„",
      "userId": "05652149574770065967"
     },
     "user_tz": -540
    },
    "id": "n9peiaywSN7B",
    "outputId": "6af407fc-3fb1-4480-8d07-555bcd2c7cd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: ë‹¹ì‹ ì€ KB ë¶€ë™ì‚° ë³´ê³ ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
      "\n",
      "ì»¨í…ìŠ¤íŠ¸: ì»¨í…ìŠ¤íŠ¸ ì˜ˆì‹œ\n",
      "\n",
      "Human: ëŒ€í™” ê¸°ë¡ ì˜ˆì‹œ1\n",
      "Human: ëŒ€í™” ê¸°ë¡ ì˜ˆì‹œ2\n",
      "Human: ì§ˆë¬¸ ì˜ˆì‹œ\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(context=\"ì»¨í…ìŠ¤íŠ¸ ì˜ˆì‹œ\", chat_history=[\"ëŒ€í™” ê¸°ë¡ ì˜ˆì‹œ1\", \"ëŒ€í™” ê¸°ë¡ ì˜ˆì‹œ2\"], question=\"ì§ˆë¬¸ ì˜ˆì‹œ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Z891npm-XOJE"
   },
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ í˜•ì‹ ë³€í™˜ í•¨ìˆ˜ ì •ì˜\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)  # ë¬¸ì„œ ë‚´ìš©ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì—°ê²°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kQvLvIB9Or7L"
   },
   "outputs": [],
   "source": [
    "# ì²´ì¸ êµ¬ì„±: ê²€ìƒ‰í•œ ë¬¸ì„œë¥¼ í”„ë¡¬í”„íŠ¸ì— ì—°ê²°í•˜ê³  ëª¨ë¸ì„ í†µí•´ ì‘ë‹µ ìƒì„±\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=lambda x: format_docs(retriever.invoke(x[\"question\"]))\n",
    "    )  # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì—°ê²°í•˜ì—¬ ì „ë‹¬\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()  # ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DM2nx8PVVQ-j"
   },
   "outputs": [],
   "source": [
    "# ëŒ€í™” ê¸°ë¡ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ë©”ëª¨ë¦¬ ì„¤ì •\n",
    "chat_history = ChatMessageHistory()\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: chat_history,  # ì„¸ì…˜ IDë³„ ëŒ€í™” ê¸°ë¡ ìƒì„±\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6267,
     "status": "ok",
     "timestamp": 1741001033972,
     "user": {
      "displayName": "ì¡°ê²½ì•„",
      "userId": "05652149574770065967"
     },
     "user_tz": -540
    },
    "id": "ISuCWtWMVQ7y",
    "outputId": "a881e418-febb-457d-d094-a8c50f651fc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB ë¶€ë™ì‚° ë³´ê³ ì„œ ì±—ë´‡ì…ë‹ˆë‹¤. ì§ˆë¬¸í•´ ì£¼ì„¸ìš”. (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)\n",
      "ì±—ë´‡: ë­ì²´ì¸ ì±—ë´‡ì„ ë§Œë“¤ê¸° ìœ„í•œ ì‹¤ìŠµì„ ì§„í–‰ ì¤‘ì´ì‹œêµ°ìš”! ë­ì²´ì¸(LLM Chain)ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì±—ë´‡ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ê°€ í•„ìš”í•©ë‹ˆë‹¤:\n",
      "\n",
      "1. **í™˜ê²½ ì„¤ì •**: Pythonê³¼ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬(ì˜ˆ: LangChain, OpenAI API ë“±)ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
      "\n",
      "2. **API í‚¤ ì„¤ì •**: OpenAIì™€ ê°™ì€ ì–¸ì–´ ëª¨ë¸ APIì˜ í‚¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
      "\n",
      "3. **ê¸°ë³¸ êµ¬ì¡° ì„¤ê³„**: ì±—ë´‡ì˜ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤. ì‚¬ìš©ì ì…ë ¥ì„ ë°›ê³ , ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” íë¦„ì„ ë§Œë“­ë‹ˆë‹¤.\n",
      "\n",
      "4. **ëŒ€í™” íë¦„ ì •ì˜**: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì„ ì–´ë–»ê²Œ ì²˜ë¦¬í• ì§€ ì •ì˜í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ì§ˆë¬¸ì— ëŒ€í•´ ë¯¸ë¦¬ ì •ì˜ëœ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "5. **í…ŒìŠ¤íŠ¸ ë° ê°œì„ **: ì±—ë´‡ì„ í…ŒìŠ¤íŠ¸í•˜ê³ , ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤.\n",
      "\n",
      "6. **ë°°í¬**: ì±—ë´‡ì„ ì›¹ì‚¬ì´íŠ¸ë‚˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì— í†µí•©í•˜ì—¬ ì‚¬ìš©ìë“¤ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë°°í¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "êµ¬ì²´ì ì¸ ì½”ë“œë‚˜ ì˜ˆì œê°€ í•„ìš”í•˜ì‹œë‹¤ë©´, ì–´ë–¤ ë¶€ë¶„ì— ëŒ€í•´ ë” ì•Œê³  ì‹¶ìœ¼ì‹ ì§€ ë§ì”€í•´ ì£¼ì‹œë©´ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤!\n",
      "ì±—ë´‡: ì‚¬ìš©ìë‹˜ê»˜ì„œ \"ë­ì²´ì¸ ì±—ëª» ë§Œë“¤ê¸°ì— ëŒ€í•´ ì‹¤ìŠµì¤‘ì…ë‹ˆë‹¤\"ë¼ê³  ë§ì”€í•˜ì…¨ìŠµë‹ˆë‹¤. ë­ì²´ì¸ ì±—ë´‡ì„ ë§Œë“¤ê¸° ìœ„í•œ ì‹¤ìŠµì„ ì§„í–‰ ì¤‘ì´ë¼ê³  í•˜ì…¨ìŠµë‹ˆë‹¤. ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì´ë‚˜ ë„ì›€ì´ í•„ìš”í•˜ì‹  ë¶€ë¶„ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!\n",
      "ì±—ë´‡: 2024 KB ë¶€ë™ì‚° ë³´ê³ ì„œì— ë”°ë¥´ë©´, ìˆ˜ë„ê¶Œ ì£¼íƒì‹œì¥ì€ ì „ë°˜ì ìœ¼ë¡œ ì¹¨ì²´ ìƒíƒœì— ìˆìœ¼ë©°, íŠ¹íˆ ê°•ë‚¨ê¶Œê³¼ ê°™ì€ ì„ í˜¸ ì§€ì—­ì—ì„œëŠ” ìƒëŒ€ì ìœ¼ë¡œ ê°•ì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ ìˆ˜ë„ê¶Œ ì£¼íƒ ë§¤ë§¤ ì „ë§ì— ëŒ€í•œ ì£¼ìš” ë‚´ìš©ì…ë‹ˆë‹¤:\n",
      "\n",
      "1. **ê±°ë˜ íšŒë³µì˜ ì–´ë ¤ì›€**: ë†’ì€ ê¸°ì¤€ê¸ˆë¦¬ì™€ ì£¼íƒ ë§¤ë§¤ê°€ê²©, DSR(ì´ë¶€ì±„ìƒí™˜ë¹„ìœ¨) ê·œì œ ë“±ìœ¼ë¡œ ì¸í•´ ë§¤ìˆ˜ìë“¤ì˜ êµ¬ë§¤ ì—¬ë ¥ì´ íšŒë³µë˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤. ë§¤ë„ìì™€ ë§¤ìˆ˜ì ê°„ì˜ í¬ë§ê°€ê²© ì°¨ì´ê°€ ì—¬ì „íˆ ì¡´ì¬í•˜ì—¬ ê±°ë˜ê°€ ìœ„ì¶•ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **ì •ë¶€ ê·œì œ ì™„í™”**: 2023ë…„ ì´í›„ ì •ë¶€ì˜ ë‹¤ì–‘í•œ ê·œì œ ì™„í™”ê°€ ì´ë£¨ì–´ì¡Œìœ¼ë©°, ì´ëŠ” ë§¤ë„ìë“¤ì˜ ê¸°ëŒ€ ì‹¬ë¦¬ë¥¼ ë†’ì´ê³  ìˆìŠµë‹ˆë‹¤. ì¬ê±´ì¶• ê·œì œ ì™„í™”ì™€ ê°™ì€ í˜¸ì¬ê°€ ì§€ì—­ë³„ë¡œ ì¡´ì¬í•˜ì§€ë§Œ, ì „ë°˜ì ì¸ ì£¼íƒ ê²½ê¸° ìœ„ì¶• ìš”ì†Œë„ ì—¬ì „íˆ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "3. **ì§€ì—­ë³„ ì°¨ë³„í™”**: ì„œìš¸ì˜ ê²½ìš°, ê°•ë‚¨êµ¬ì™€ ì„œì´ˆêµ¬ ë“± ì„ í˜¸ ì§€ì—­ì—ì„œëŠ” ê¸ì •ì ì¸ ê¸°ëŒ€ê°ì´ ìœ ì§€ë˜ê³  ìˆìœ¼ë‚˜, ë§ˆí¬êµ¬ì™€ ìš©ì‚°êµ¬ ë“±ì—ì„œëŠ” ë§¤ë„ìì™€ ë§¤ìˆ˜ì ê°„ì˜ ê°„ê·¹ì´ ì—¬ì „íˆ í¬ê³ , ì‹¤ì œ ë§¤ìˆ˜ ê°€ë‹´ ì—¬ë¶€ê°€ ì¤‘ìš”í•œ ìš”ì†Œë¡œ ì‘ìš©í•  ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "4. **ì „ì„¸ ì‹œì¥ê³¼ì˜ ì—°ê´€ì„±**: ì „ì„¸ê°€ê²© í•˜ë½ì´ ë§¤ë§¤ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ë„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ëŒ€ê·œëª¨ ë‹¨ì§€ì˜ ì…ì£¼ê°€ ì˜ˆì •ë˜ì–´ ìˆëŠ” ì§€ì—­ì—ì„œëŠ” ì „ì„¸ê°€ê²© í•˜ë½ì´ ë§¤ë§¤ê°€ê²©ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "5. **ì „ë°˜ì ì¸ ì „ë§**: í•˜ë°˜ê¸° ì´í›„ ê±°ë˜ ì¹¨ì²´ í˜„ìƒì´ ë‹¤ì†Œ ì™„í™”ë  ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë‚˜, ì „ë°˜ì ì¸ íšŒë³µì„¸ëŠ” ì œí•œì ì¼ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ë§¤ë„ìì™€ ë§¤ìˆ˜ì ê°„ì˜ í¬ë§ê°€ê²© ê²©ì°¨ ì¶•ì†Œì™€ ë§¤ìˆ˜ì„¸ íšŒë³µì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ ìš”ì†Œë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•  ë•Œ, ìˆ˜ë„ê¶Œ ì£¼íƒì‹œì¥ì€ ë‹¨ê¸°ì ìœ¼ë¡œëŠ” ì–´ë ¤ìš´ ìƒí™©ì´ ì§€ì†ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. ì¶”ê°€ì ì¸ ì§ˆë¬¸ì´ë‚˜ íŠ¹ì • ì§€ì—­ì— ëŒ€í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "# ì±—ë´‡ ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜\n",
    "def chat_with_bot():\n",
    "    session_id = \"user_session\"\n",
    "    print(\"KB ë¶€ë™ì‚° ë³´ê³ ì„œ ì±—ë´‡ì…ë‹ˆë‹¤. ì§ˆë¬¸í•´ ì£¼ì„¸ìš”. (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)\")\n",
    "    while True:\n",
    "        user_input = input(\"ì‚¬ìš©ì: \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        response = chain_with_memory.invoke(\n",
    "            {\"question\": user_input},\n",
    "            {\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "\n",
    "        print(\"ì±—ë´‡:\", response)\n",
    "\n",
    "# ë©”ì¸ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    chat_with_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "CYkPcYod5tAZ"
   },
   "outputs": [],
   "source": [
    "# <ìŠ¤íŠ¸ë¦¼ë¦¿ ì ìš©>\n",
    "\n",
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "# %%capture --no-stderr\n",
    "# !pip install streamlit pyngrok\n",
    "\n",
    "# ngrok ì¸ì¦í‚¤ ì„¤ì •\n",
    "# !ngrok config add-authtoken 2tnwXk5jQ5uWVBnwy3Ou8Mdmu8v_5eAtrJHxGZRiGGKqsqHsh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "app.py íŒŒì¼ì„ ìƒì„±í•˜ê³  ì•„ë˜ ì½”ë“œë¥¼ ë³µì‚¬í•´ì„œ ë¶™ì—¬ë„£ê¸°\n",
    "```\n",
    "# ìŠ¤íŠ¸ë¦¼ë¦¿ì•± ì‹¤í–‰\n",
    "streamlit run app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCoW44bz764Z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 12:41:48.455 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 12:41:48.455 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 12:41:48.529 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/maui/Repository/personal/books/book-rag-master/.venv/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-05-18 12:41:48.529 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 12:41:48.529 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 12:41:48.529 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 12:41:48.529 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 12:41:48.530 Session state does not function when running a script without `streamlit run`\n",
      "2025-05-18 12:41:48.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 12:41:48.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 12:41:48.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 12:41:48.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 12:41:48.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 12:41:48.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 12:41:48.531 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 12:41:48.531 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# app.py\n",
    "import os\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv(\"/content/.env\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# PDF ì²˜ë¦¬ í•¨ìˆ˜\n",
    "@st.cache_resource\n",
    "def process_pdf():\n",
    "    # loader = PyPDFLoader(\"/content/drive/MyDrive/langchain-tutorial/Ch02. RAG/Data/2024_KB_ë¶€ë™ì‚°_ë³´ê³ ì„œ_ìµœì¢….pdf\")\n",
    "    loader = PyPDFLoader(\"./Data/2024_KB_ë¶€ë™ì‚°_ë³´ê³ ì„œ_ìµœì¢….pdf\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "# ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”\n",
    "@st.cache_resource\n",
    "def initialize_vectorstore():\n",
    "    chunks = process_pdf()\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "    return Chroma.from_documents(chunks, embeddings)\n",
    "\n",
    "# ì²´ì¸ ì´ˆê¸°í™”\n",
    "@st.cache_resource\n",
    "def initialize_chain():\n",
    "    vectorstore = initialize_vectorstore()\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    template = \"\"\"ë‹¹ì‹ ì€ KB ë¶€ë™ì‚° ë³´ê³ ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "    ì»¨í…ìŠ¤íŠ¸: {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", template),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "\n",
    "    model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, openai_api_key=api_key)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    base_chain = (\n",
    "        RunnablePassthrough.assign(\n",
    "            context=lambda x: format_docs(retriever.invoke(x[\"question\"]))\n",
    "        )\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return RunnableWithMessageHistory(\n",
    "        base_chain,\n",
    "        lambda session_id: ChatMessageHistory(),\n",
    "        input_messages_key=\"question\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "    )\n",
    "\n",
    "# Streamlit UI\n",
    "def main():\n",
    "    st.set_page_config(page_title=\"KB ë¶€ë™ì‚° ë³´ê³ ì„œ ì±—ë´‡\", page_icon=\"ğŸ \")\n",
    "    st.title(\"ğŸ  KB ë¶€ë™ì‚° ë³´ê³ ì„œ AI ì–´ë“œë°”ì´ì €\")\n",
    "    st.caption(\"2024 KB ë¶€ë™ì‚° ë³´ê³ ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ\")\n",
    "\n",
    "    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "\n",
    "    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬\n",
    "    if prompt := st.chat_input(\"ë¶€ë™ì‚° ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”\"):\n",
    "        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(prompt)\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        # ì²´ì¸ ì´ˆê¸°í™”\n",
    "        chain = initialize_chain()\n",
    "\n",
    "        # AI ì‘ë‹µ ìƒì„±\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            with st.spinner(\"ë‹µë³€ ìƒì„± ì¤‘...\"):\n",
    "                response = chain.invoke(\n",
    "                    {\"question\": prompt},\n",
    "                    {\"configurable\": {\"session_id\": \"streamlit_session\"}}\n",
    "                )\n",
    "                st.markdown(response)\n",
    "\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yb_DoR-_v5-o",
    "outputId": "378bf7ab-adbd-414c-8a08-140f22665798"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•± ì ‘ì† URL: NgrokTunnel: \"https://d628-34-23-186-167.ngrok-free.app\" -> \"http://localhost:8501\"\n",
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.23.186.167:8501\u001b[0m\n",
      "\u001b[0m\n",
      "/content/app.py:12: LangChainDeprecationWarning: Importing ChatMessageHistory from langchain.memory is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.memory import ChatMessageHistory\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.chat_message_histories import ChatMessageHistory\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
      "  from langchain.memory import ChatMessageHistory\n",
      "/content/app.py:12: LangChainDeprecationWarning: Importing ChatMessageHistory from langchain.memory is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.memory import ChatMessageHistory\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.chat_message_histories import ChatMessageHistory\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
      "  from langchain.memory import ChatMessageHistory\n",
      "/content/app.py:12: LangChainDeprecationWarning: Importing ChatMessageHistory from langchain.memory is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.memory import ChatMessageHistory\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.chat_message_histories import ChatMessageHistory\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
      "  from langchain.memory import ChatMessageHistory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pyngrok.process.ngrok:t=2025-03-03T13:41:11+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-5a6dbb55-2eea-44b3-9a82-4930899ee63a acceptErr=\"failed to accept connection: Listener closed\"\n",
      "WARNING:pyngrok.process.ngrok:t=2025-03-03T13:41:11+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8501-5a6dbb55-2eea-44b3-9a82-4930899ee63a err=\"failed to start tunnel: session closed\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m  Stopping...\u001b[0m\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# í„°ë„ë§ ë° ì‹¤í–‰\n",
    "from pyngrok import ngrok\n",
    "\n",
    "public_url = ngrok.connect(8501)  # Streamlit ê¸°ë³¸ í¬íŠ¸\n",
    "print(\"ì•± ì ‘ì† URL:\", public_url)\n",
    "!streamlit run /content/app.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RL2sLCHnvc2I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPAxmEMI5LsnMrs7PUV2V6o",
   "provenance": [
    {
     "file_id": "1uyUNS30ebKj-Z2y9LfynVxat8Z-kGi4j",
     "timestamp": 1730783748682
    },
    {
     "file_id": "121A8jqwXbjJbUfwI0WuV53th7o0frtkK",
     "timestamp": 1730646342102
    },
    {
     "file_id": "1pdlCEHwvXT4Ljv9VGBciaemVHcBIiyMn",
     "timestamp": 1730563801037
    },
    {
     "file_id": "17pNawQlJCc7SQHqPznYT2QVIwvB4kjzI",
     "timestamp": 1729507273500
    },
    {
     "file_id": "1hYMuG0nH6xN6PV1yYXHF5rojjQIqhmi0",
     "timestamp": 1729491300245
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
